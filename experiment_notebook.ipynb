import os, csv, argparse, textwrap
import numpy as np
import torch
import torch.nn as nn
from PIL import Image
import matplotlib.pyplot as plt

from datasets import load_dataset
from bs4 import BeautifulSoup
from torchvision import transforms

from transformers import AutoTokenizer
from transformers import AutoTokenizer as GPT2Tokenizer
from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler

# -----------------------------------------------------------------
  CLI
# ------------------------------------------------------------------
def parse_args():
    p = argparse.ArgumentParser("Multimodal Decoder Visualize + Ablation")
    p.add_argument("--root", type=str, default="/content/drive/MyDrive/Mutimodal_Sequential_prediction",
                   help="Root folder containing checkpoints/ and results/")
    p.add_argument("--sd_model_id", type=str, default="runwayml/stable-diffusion-v1-5")
    p.add_argument("--decoder_ckpt", type=str, default="Multimodal_Decoders.pt",
                   help="Decoder ckpt filename inside checkpoints/")
    p.add_argument("--cache_file", type=str, default="decoder_cache.pt",
                   help="Cache filename inside checkpoints/")
    p.add_argument("--split", type=str, default="train", help="train or test (use same split as your cache)")
    p.add_argument("--idx", type=int, default=0, help="Index to visualize")
    p.add_argument("--mode", type=str, choices=["visualize", "ablate"], default="visualize")
    p.add_argument("--max_len", type=int, default=64)
    p.add_argument("--steps", type=int, default=20)
    p.add_argument("--strength", type=float, default=0.05)
    p.add_argument("--cfg", type=float, default=1.0)

    # Ablation grids (comma-separated)
    p.add_argument("--idx_list", type=str, default="0,5,10")
    p.add_argument("--steps_list", type=str, default="10,20,30")
    p.add_argument("--strengths", type=str, default="0.02,0.05,0.10")
    p.add_argument("--cfg_scales", type=str, default="1.0,3.0,5.0,7.0")

    p.add_argument("--no_show", action="store_true", help="Do not show plots (useful for batch runs)")
    return p.parse_args()

# --------------------------------------------------------------------------------
 Utils
# ---------------------------------------------------------------------------------
def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)
    return path

def parse_gdi(text):
    soup = BeautifulSoup(text, "html.parser")
    return [x.get_text().strip() for x in soup.find_all("gdi")]

def wrap_text(s, width=60, max_lines=6):
    s = (s or "").replace("\n", " ").strip()
    lines = textwrap.wrap(s, width=width)
    if len(lines) > max_lines:
        lines = lines[:max_lines]
        lines[-1] = lines[-1] + "â€¦"
    return "\n".join(lines)

def to_u8(img_3hw):
    x = img_3hw.detach().float().cpu().clamp(0,1)
    return (x.permute(1,2,0).numpy() * 255).round().clip(0,255).astype(np.uint8)

def to_minus1_1(img01):
    return img01 * 2.0 - 1.0

# ---------------------------------------------------------------------------------
Dataset 
# -----------------------------------------------------------------------------------
tokenizer_bert = AutoTokenizer.from_pretrained("bert-base-uncased")
img_tf_224 = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])
img_tf_512 = transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])

class StoryDataset(torch.utils.data.Dataset):
    """
    Keys:
      imgs224: (3,3,224,224)
      ids:    (3,L)
      mask:   (3,L)
      texts:  [t1,t2,t3,t4]
      gt_img512: (3,512,512)
    """
    def __init__(self, split="train", max_len=64):
        self.ds = load_dataset("daniel3303/StoryReasoning", split=split)
        self.max_len = max_len

    def __len__(self): return len(self.ds)

    def __getitem__(self, i):
        s = self.ds[i]
        texts = (parse_gdi(s["story"]) + [""]*4)[:4]

        imgs224 = torch.stack([img_tf_224(s["images"][j]) for j in range(4)])  # (4,3,224,224)
        gt_img512 = img_tf_512(s["images"][3])                                 # (3,512,512)

        ctx = tokenizer_bert(
            texts[:3],
            padding="max_length",
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )

        return {
            "imgs224": imgs224[:3],
            "ids": ctx["input_ids"],
            "mask": ctx["attention_mask"],
            "texts": texts,
            "gt_img512": gt_img512
        }

# ----------------------------
Text decoder 
# ----------------------------
gpt2_tok = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_tok.pad_token = gpt2_tok.eos_token
VOCAB_SIZE = gpt2_tok.vocab_size

class DecoderText(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_layers=3, n_heads=8, max_len=64):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        self.pos   = nn.Embedding(max_len, d_model)

        layer = nn.TransformerDecoderLayer(
            d_model=d_model, nhead=n_heads, batch_first=True, activation="gelu"
        )
        self.dec = nn.TransformerDecoder(layer, num_layers=n_layers)
        self.out = nn.Linear(d_model, vocab_size)
        self.max_len = max_len

    def forward(self, input_ids, memory_tokens):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device).unsqueeze(0)
        x = self.embed(input_ids) + self.pos(pos)
        causal = nn.Transformer.generate_square_subsequent_mask(L, device=input_ids.device)
        h = self.dec(x, memory_tokens, tgt_mask=causal)
        return self.out(h)

@torch.no_grad()
def generate_text_from_z(dec_text, z_tokens, device, dtype, max_len=64, temperature=0.9, top_k=50):
    """
    z_tokens: (T,512) or (1,T,512)
    """
    if z_tokens.dim() == 2:
        z = z_tokens.unsqueeze(0)
    else:
        z = z_tokens
    z = z.to(device, dtype=dtype)

    start_id = gpt2_tok.eos_token_id
    tokens = torch.tensor([[start_id]], device=device, dtype=torch.long)

    for _ in range(max_len - 1):
        logits = dec_text(tokens, z)[:, -1, :] / max(temperature, 1e-6)

        if top_k and top_k > 0:
            v, ix = torch.topk(logits, k=top_k, dim=-1)
            probs = torch.softmax(v, dim=-1)
            nxt_local = torch.multinomial(probs, num_samples=1)
            nxt = ix.gather(-1, nxt_local)
        else:
            nxt = torch.argmax(logits, dim=-1, keepdim=True)

        tokens = torch.cat([tokens, nxt], dim=1)
        if int(nxt.item()) == int(gpt2_tok.eos_token_id):
            break

    out = gpt2_tok.decode(tokens[0].tolist(), skip_special_tokens=True).strip()
    return out if out else "<empty prediction>"

# ------------------------------------------------------
 VAE encode/decode (dtype-safe)
# ---------------------------------------------------
@torch.no_grad()
def encode_latents(vae, img01_1x3, device, dtype):
    x = img01_1x3.to(device, dtype=dtype)
    x = to_minus1_1(x)
    lat = vae.encode(x).latent_dist.sample() * 0.18215
    return lat

@torch.no_grad()
def decode_latents(vae, lat_1x4, device, dtype):
    lat = lat_1x4.to(device, dtype=dtype)
    img = vae.decode(lat / 0.18215).sample
    return ((img + 1) / 2).clamp(0, 1)

# -----------------------------------------
 UNet GT-like reconstruction with CFG
# -----------------------------------------
@torch.no_grad()
def gt_like_reconstruct_unet_cfg(
    unet, latent_mapper, vae, ddim,
    z_tokens, gt_lat_1x4,
    device, dtype,
    steps=20, strength=0.05,
    cfg_scale=1.0
):
    z = z_tokens.unsqueeze(0).to(device, dtype=dtype)      # (1,T,512)
    cond = latent_mapper(z).to(device, dtype=dtype)        # (1,T,768)
    uncond = torch.zeros_like(cond)

    lat = gt_lat_1x4.to(device, dtype=dtype)

    ddim.set_timesteps(steps, device=device)
    start = int(strength * steps)
    start = max(0, min(start, steps - 1))
    t_start = ddim.timesteps[start]

    noise = torch.randn_like(lat)
    lat = ddim.add_noise(lat, noise, t_start)

    for t in ddim.timesteps[start:]:
        t_batch = torch.full((1,), int(t), device=device, dtype=torch.long)

        eps_cond = unet(lat, t_batch, encoder_hidden_states=cond).sample
        if cfg_scale is None or float(cfg_scale) == 1.0:
            eps = eps_cond
        else:
            eps_uncond = unet(lat, t_batch, encoder_hidden_states=uncond).sample
            eps = eps_uncond + float(cfg_scale) * (eps_cond - eps_uncond)

        lat = ddim.step(eps, t, lat).prev_sample

    pred_img = decode_latents(vae, lat, device, dtype)  # (1,3,512,512)
    return pred_img

# ----------------------------
Load decoder models from ckpt (keys: unet, latent_mapper, dec_text, config)
# ----------------------------
def build_decoder_models_from_ckpt(dec_ckpt_path, sd_model_id, device, dtype, max_len):
    ck = torch.load(dec_ckpt_path, map_location="cpu")
    print("Decoder ckpt keys:", ck.keys())

    unet = UNet2DConditionModel.from_pretrained(sd_model_id, subfolder="unet")
    latent_mapper = nn.Sequential(nn.Linear(512, 768), nn.LayerNorm(768))
    dec_text = DecoderText(VOCAB_SIZE, d_model=512, n_layers=3, n_heads=8, max_len=max_len)

    # Load weights
    unet.load_state_dict(ck["unet"], strict=False)
    latent_mapper.load_state_dict(ck["latent_mapper"], strict=True)
    dec_text.load_state_dict(ck["dec_text"], strict=True)

    vae = AutoencoderKL.from_pretrained(sd_model_id, subfolder="vae")
    ddim = DDIMScheduler(num_train_timesteps=1000)

    unet = unet.to(device, dtype=dtype).eval()
    latent_mapper = latent_mapper.to(device, dtype=dtype).eval()
    dec_text = dec_text.to(device, dtype=dtype).eval()
    vae = vae.to(device, dtype=dtype).eval()

    for m in [unet, latent_mapper, dec_text, vae]:
        for p in m.parameters(): p.requires_grad = False

    return unet, latent_mapper, dec_text, vae, ddim

# ------------------------------------------------
  Cache loader (list with z_pred, gt_latent, text)
# ------------------------------------------------
def load_cache(cache_path):
    cache = torch.load(cache_path, map_location="cpu")
    assert isinstance(cache, list), f"Cache must be list, got {type(cache)}"
    assert "z_pred" in cache[0], "Cache items must contain 'z_pred'"
    return cache

def get_z_tokens(cache, idx):
    z = cache[idx]["z_pred"]
    return z if isinstance(z, torch.Tensor) else torch.tensor(z)

# -----------------------------------
CLIP scorer (optional but recommended)
# -----------------------------------
class ClipScorer:
    def __init__(self, device):
        self.device = device
        self.enabled = False
        try:
            from transformers import CLIPProcessor, CLIPModel
            self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device).eval()
            self.proc  = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            self.enabled = True
        except Exception as e:
            print("CLIP disabled (install transformers/clip deps). Reason:", str(e)[:160])

    @torch.no_grad()
    def image_text_cosine(self, pil_image, text: str) -> float:
        if not self.enabled:
            return float("nan")
        import torch.nn.functional as F
        inp = self.proc(images=pil_image, text=[text], return_tensors="pt", padding=True).to(self.device)
        out = self.model(**inp)
        img = F.normalize(out.image_embeds, dim=-1)
        txt = F.normalize(out.text_embeds, dim=-1)
        return float((img * txt).sum(dim=-1).item())

# ---------------------------------------------
visualization (saves + returns pred_pil for CLIP)
# ---------------------------------------------
def draw_text(ax, title, text, fontsize=9):
    ax.axis("off")
    ax.set_title(title, fontsize=11)
    ax.text(
        0.0, 1.0, wrap_text(text, width=60, max_lines=6),
        ha="left", va="top", fontsize=fontsize,
        transform=ax.transAxes
    )
def visualize_5panel_with_z(
    idx, z_tokens, split, max_len,
    unet, latent_mapper, dec_text, vae, ddim,
    device, dtype,
    steps=20, strength=0.05, cfg_scale=1.0,
    save_path=None, show=True
):
    ds = StoryDataset(split=split, max_len=max_len)
    item = ds[idx]

    imgs224 = item["imgs224"]     # (3,3,224,224)
    texts   = item["texts"]       # 4 strings
    gt_img512 = item["gt_img512"] # (3,512,512)

    gt_txt4 = str(texts[3])

    # GT latent from GT image
    gt_lat = encode_latents(vae, gt_img512.unsqueeze(0), device, dtype)     # (1,4,64,64)
    gt_img = decode_latents(vae, gt_lat, device, dtype)[0]                 # (3,512,512)

    # Pred image (GT-like with cfg)
    pred_img = gt_like_reconstruct_unet_cfg(
        unet, latent_mapper, vae, ddim,
        z_tokens.to(torch.float32), gt_lat,
        device, dtype,
        steps=steps, strength=strength,
        cfg_scale=cfg_scale
    )[0]  # (3,512,512)

    # Pred text
    pred_txt4 = generate_text_from_z(dec_text, z_tokens.to(torch.float32), device, dtype, max_len=max_len)

    # Make pred PIL for CLIP
    pred_pil = Image.fromarray(to_u8(pred_img))

    # Plot
    plt.figure(figsize=(18, 7))

    titles = ["Image 1", "Image 2", "Image 3", "GT Image 4", f"Pred (steps={steps}, str={strength:.3f}, cfg={cfg_scale:.2f})"]
    imgs_top = [imgs224[0], imgs224[1], imgs224[2], gt_img, pred_img]

    for i in range(5):
        ax = plt.subplot(2,5,i+1)
        ax.imshow(Image.fromarray(to_u8(imgs_top[i])))
        ax.axis("off")
        ax.set_title(titles[i], fontsize=12)

    ax6  = plt.subplot(2,5,6);  draw_text(ax6,  "Text 1",      str(texts[0]))
    ax7  = plt.subplot(2,5,7);  draw_text(ax7,  "Text 2",      str(texts[1]))
    ax8  = plt.subplot(2,5,8);  draw_text(ax8,  "Text 3",      str(texts[2]))
    ax9  = plt.subplot(2,5,9);  draw_text(ax9,  "GT Text 4",   gt_txt4)
    ax10 = plt.subplot(2,5,10); draw_text(ax10, "Pred Text 4", pred_txt4)

    plt.tight_layout()

    if save_path:
        ensure_dir(os.path.dirname(save_path))
        plt.savefig(save_path, dpi=200, bbox_inches="tight")

    if show:
        plt.show()
    else:
        plt.close()

    return {
        "save_path": save_path,
        "pred_text4": pred_txt4,
        "gt_text4": gt_txt4,
        "pred_pil": pred_pil
    }

# ----------------------------
Ablation runner
# ----------------------------
def run_ablation(
    idx_list, steps_list, strengths, cfg_scales,
    cache, split, max_len,
    unet, latent_mapper, dec_text, vae, ddim,
    device, dtype,
    results_dir, show=False
):
    panels_dir = ensure_dir(os.path.join(results_dir, "ablations"))
    csv_path = os.path.join(results_dir, "ablation_metrics.csv")

    clip = ClipScorer(device=device)

    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "idx","split","steps","strength","cfg_scale",
            "panel_path",
            "clip_predimg_predtxt","clip_predimg_gttxt"
        ])
        writer.writeheader()

        for idx in idx_list:
            z_tokens = get_z_tokens(cache, idx)
            for steps in steps_list:
                for strength in strengths:
                    for cfg_scale in cfg_scales:
                        panel_path = os.path.join(
                            panels_dir, f"idx{idx}_steps{steps}_str{strength:.3f}_cfg{cfg_scale:.2f}.png"
                        )

                        out = visualize_5panel_with_z(
                            idx=idx, z_tokens=z_tokens, split=split, max_len=max_len,
                            unet=unet, latent_mapper=latent_mapper, dec_text=dec_text, vae=vae, ddim=ddim,
                            device=device, dtype=dtype,
                            steps=steps, strength=strength, cfg_scale=cfg_scale,
                            save_path=panel_path, show=show
                        )

                        pred_pil = out["pred_pil"]
                        pred_txt = out["pred_text4"]
                        gt_txt   = out["gt_text4"]

                        clip_pp = clip.image_text_cosine(pred_pil, pred_txt)
                        clip_pg = clip.image_text_cosine(pred_pil, gt_txt)

                        writer.writerow({
                            "idx": idx,
                            "split": split,
                            "steps": steps,
                            "strength": strength,
                            "cfg_scale": cfg_scale,
                            "panel_path": panel_path,
                            "clip_predimg_predtxt": clip_pp,
                            "clip_predimg_gttxt": clip_pg,
                        })

                        print(f"[ablate] idx={idx} steps={steps} str={strength:.3f} cfg={cfg_scale:.2f} | "
                              f"CLIP(pred,predtxt)={clip_pp:.4f} CLIP(pred,gttxt)={clip_pg:.4f}")

    return csv_path

# ---------------------------------------------------------
  Main
# ---------------------------------------------------------
def main():
    args = parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    dtype  = torch.float16 if device == "cuda" else torch.float32

    ckpt_dir = os.path.join(args.root, "checkpoints")
    results_dir = ensure_dir(os.path.join(args.root, "results"))
    panels_dir  = ensure_dir(os.path.join(results_dir, "panels"))

    dec_ckpt_path = os.path.join(ckpt_dir, args.decoder_ckpt)
    cache_path    = os.path.join(ckpt_dir, args.cache_file)

    assert os.path.exists(dec_ckpt_path), f"Missing decoder ckpt: {dec_ckpt_path}"
    assert os.path.exists(cache_path), f"Missing cache file: {cache_path}"

    print("DEVICE:", device, "| DTYPE:", dtype)
    print("Decoder ckpt:", dec_ckpt_path)
    print("Cache:", cache_path)

    cache = load_cache(cache_path)
    print("Cache length:", len(cache), "| keys:", cache[0].keys())

    unet, latent_mapper, dec_text, vae, ddim = build_decoder_models_from_ckpt(
        dec_ckpt_path, args.sd_model_id, device, dtype, args.max_len
    )

    if args.mode == "visualize":
        z_tokens = get_z_tokens(cache, args.idx)
        save_path = os.path.join(panels_dir, f"visualize_5panel_idx{args.idx}.png")

        out = visualize_5panel_with_z(
            idx=args.idx, z_tokens=z_tokens, split=args.split, max_len=args.max_len,
            unet=unet, latent_mapper=latent_mapper, dec_text=dec_text, vae=vae, ddim=ddim,
            device=device, dtype=dtype,
            steps=args.steps, strength=args.strength, cfg_scale=args.cfg,
            save_path=save_path,
            show=(not args.no_show)
        )
        print("Saved:", out["save_path"])
        print("GT text4:", out["gt_text4"])
        print("Pred text4:", out["pred_text4"])

    else:
        idx_list   = [int(x.strip()) for x in args.idx_list.split(",") if x.strip()]
        steps_list = [int(x.strip()) for x in args.steps_list.split(",") if x.strip()]
        strengths  = [float(x.strip()) for x in args.strengths.split(",") if x.strip()]
        cfg_scales = [float(x.strip()) for x in args.cfg_scales.split(",") if x.strip()]

        csv_path = run_ablation(
            idx_list=idx_list,
            steps_list=steps_list,
            strengths=strengths,
            cfg_scales=cfg_scales,
            cache=cache,
            split=args.split,
            max_len=args.max_len,
            unet=unet,
            latent_mapper=latent_mapper,
            dec_text=dec_text,
            vae=vae,
            ddim=ddim,
            device=device,
            dtype=dtype,
            results_dir=results_dir,
            show=(not args.no_show)
        )
        print("Ablation complete. CSV:", csv_path)
        print("Ablation panels in:", os.path.join(results_dir, "ablations"))

if __name__ == "__main__":
    main()
